{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\" id=\"heading\">Sentiment Analysis of Reddit Data using Reddit API</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( 🤗 the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the session, you will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- know how to work with APIs\n",
    "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
    "- understand what a `pipeline` object is in HuggingFace\n",
    "- perform sentiment analysis using `pipeline`\n",
    "- run a python script in command line and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of each task, commit* the work into the repository you created before the assignment\n",
    "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your repository you created before the assignment\n",
    "- Submit the link to the notebook in Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task I: Instantiate a Reddit API Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please ensure you've ran all the cells in the `imports.ipynb`, located [here](https://github.com/FourthBrain/MLE-8/blob/main/assignments/week-3-analyze-sentiment-subreddit/imports.ipynb), to make sure you have all the required packages for today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2. Create a new app on Reddit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a Reddit account if you don't have one, log into your account.\n",
    "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
    "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect uri** ( The redirect URI is where the user is sent after they've granted OAuth access to your application (more info [here](https://github.com/reddit-archive/reddit/wiki/OAuth2)) For our purpose, you can enter some random url, e.g., www.google.com; as shown below.\n",
    "\n",
    "\n",
    "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
    "- Jot down `client_id` (left upper corner) and `client_secret` \n",
    "\n",
    "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
    "    \n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "- Create `secrets_reddit.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"client_id\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"secret_id\"\n",
    "    REDDIT_API_USER_AGENT = \"any string except bot; ex. My User Agent\"\n",
    "    ```\n",
    "- Add `secrets_reddit.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Instantiate a `Reddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/mattrobinson/fourth-brain/MLE-10/assignments/week-03-reddit-sentiment-analysis/nb', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/Users/mattrobinson/Library/Python/3.8/lib/python/site-packages', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages', '/opt/anaconda3/envs/reddit-sa/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/opt/anaconda3/envs/reddit-sa/lib/python3.8/site-packages')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import SubredditHelper\n",
    "import reddit_secrets\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_secrets.REDDIT_API_CLIENT_ID,\n",
    "    client_secret=reddit_secrets.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent=reddit_secrets.REDDIT_API_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0x1059c1be0>\n"
     ]
    }
   ],
   "source": [
    "print(reddit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Instantiate a `subreddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected output you will see ar from `r/machinelearning` unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MESSAGE_PREFIX', 'STR_FIELD', 'VALID_TIME_FILTERS', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_to_fancypants', '_create_or_update', '_fetch', '_fetch_data', '_fetch_info', '_fetched', '_kind', '_parse_xml_response', '_path', '_prepare', '_read_and_post_media', '_reddit', '_reset_attributes', '_safely_add_arguments', '_submission_class', '_submit_media', '_subreddit_collections_class', '_subreddit_list', '_upload_inline_media', '_upload_media', '_url_parts', '_validate_gallery', '_validate_inline_media', '_validate_time_filter', 'banned', 'collections', 'comments', 'contributor', 'controversial', 'display_name', 'emoji', 'filters', 'flair', 'fullname', 'gilded', 'hot', 'message', 'mod', 'moderator', 'modmail', 'muted', 'new', 'parse', 'post_requirements', 'quaran', 'random', 'random_rising', 'rising', 'rules', 'search', 'sticky', 'stream', 'stylesheet', 'submit', 'submit_gallery', 'submit_image', 'submit_poll', 'submit_video', 'subscribe', 'top', 'traffic', 'unsubscribe', 'widgets', 'wiki']\n"
     ]
    }
   ],
   "source": [
    "subreddit_name = \"machinelearning\"\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "print(dir(subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the display name of the subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machinelearning\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "ml_subreddit = subreddit.display_name\n",
    "print(ml_subreddit)\n",
    "# >> machinelearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    machinelearning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about its title, is it different from the display name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "subreddit.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Yes, the subreddit title attribute is formatted as a Proper Noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    Machine Learning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Print out the description of the subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\\n--------\\n+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\\n--------\\n+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\\n--------\\n+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\\n--------\\n+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\\n--------\\n***[@slashML on Twitter](https://twitter.com/slashML)***\\n--------\\n***[Chat with us on Slack](https://join.slack.com/t/rml-talk/shared_invite/enQtNjkyMzI3NjA2NTY2LWY0ZmRjZjNhYjI5NzYwM2Y0YzZhZWNiODQ3ZGFjYmI2NTU3YjE1ZDU5MzM2ZTQ4ZGJmOTFmNWVkMzFiMzVhYjg)***\\n--------\\n**Beginners:**\\n--------\\nPlease have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\\n\\n[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\\n\\nFor Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\\n\\nFor career related questions, visit /r/cscareerquestions/\\n\\n--------\\n\\n[Advanced Courses (2016)](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\\n\\n[Advanced Courses (2020)](https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/)\\n\\n--------\\n**AMAs:**\\n\\n[Pluribus Poker AI Team 7/19/2019](https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/)\\n\\n[DeepMind AlphaStar team (1/24//2019)](https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/)\\n\\n[Libratus Poker AI Team (12/18/2017)]\\n(https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/)\\n\\n[DeepMind AlphaGo Team (10/19/2017)](https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/)\\n\\n[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\\n\\n[Google Brain Team (8/11/2016)]\\n(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\\n\\n[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\\n\\n[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\\n\\n[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\\n\\n[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\\n\\n[Jürgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\\n\\n[Geoffrey Hinton (11/10/2014)]\\n(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\\n\\n[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\\n\\n[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\\n\\n[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\\n\\n--------\\nRelated Subreddit :\\n\\n* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\\n\\n* [Statistics](http://www.reddit.com/r/statistics)\\n\\n* [Computer Vision](http://www.reddit.com/r/computervision)\\n\\n* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\\n\\n* [NLP] (http://www.reddit.com/r/LanguageTechnology)\\n\\n* [ML Questions] (http://www.reddit.com/r/MLQuestions)\\n\\n* /r/MLjobs and /r/BigDataJobs\\n\\n* /r/datacleaning\\n\\n* /r/DataScience\\n\\n* /r/scientificresearch\\n\\n* /r/artificial'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>\n",
    "\n",
    "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
    "    --------\n",
    "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
    "    --------\n",
    "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
    "    --------\n",
    "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
    "    --------\n",
    "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task II: Parse comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. Top Posts of All Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# try run this line, what do you see? press q once you are done\n",
    "?subreddit.top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Project] From books to presentations in 10s w...</td>\n",
       "      <td>cyrildiagne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[D] A Demo from 1993 of 32-year-old Yann LeCun...</td>\n",
       "      <td>TheInsaneApp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[R] First Order Motion Model applied to animat...</td>\n",
       "      <td>programmerChilli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[N] AI can turn old photos into moving Images ...</td>\n",
       "      <td>TheInsaneApp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[D] This AI reveals how much time politicians ...</td>\n",
       "      <td>TheInsaneApp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[D] Types of Machine Learning Papers</td>\n",
       "      <td>TheInsaneApp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[D] The machine learning community has a toxic...</td>\n",
       "      <td>yusuf-bengio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I made a robot that punishes me if it detects ...</td>\n",
       "      <td>_ayushp_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Project] NEW PYTHON PACKAGE: Sync GAN Art to ...</td>\n",
       "      <td>mencil47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[P] Using oil portraits and First Order Model ...</td>\n",
       "      <td>Enguzelharf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Post            Author\n",
       "0  [Project] From books to presentations in 10s w...       cyrildiagne\n",
       "1  [D] A Demo from 1993 of 32-year-old Yann LeCun...      TheInsaneApp\n",
       "2  [R] First Order Motion Model applied to animat...  programmerChilli\n",
       "3  [N] AI can turn old photos into moving Images ...      TheInsaneApp\n",
       "4  [D] This AI reveals how much time politicians ...      TheInsaneApp\n",
       "5               [D] Types of Machine Learning Papers      TheInsaneApp\n",
       "6  [D] The machine learning community has a toxic...      yusuf-bengio\n",
       "7  I made a robot that punishes me if it detects ...          _ayushp_\n",
       "8  [Project] NEW PYTHON PACKAGE: Sync GAN Art to ...          mencil47\n",
       "9  [P] Using oil portraits and First Order Model ...       Enguzelharf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top ten posts (it's a generator)\n",
    "top_ten_gen = reddit.subreddit(f\"{ml_subreddit}\").top(time_filter=\"all\",limit=10)\n",
    "\n",
    "# init list for posts\n",
    "posts = []\n",
    "\n",
    "# get each post's title and author\n",
    "for post in top_ten_gen:\n",
    "    posts.append(\n",
    "        (post.title,post.author)\n",
    "    ) \n",
    "    \n",
    "# dataframe for a more neat display\n",
    "posts_df = pd.DataFrame(posts,index = [x for x in range(0,len(posts))], columns=[\"Post\",\"Author\"])\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details> <summary>Expected output:</summary>\n",
    "\n",
    "    [Project] From books to presentations in 10s with AR + ML\n",
    "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
    "    [R] First Order Motion Model applied to animate paintings\n",
    "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
    "    [D] This AI reveals how much time politicians stare at their phone at work\n",
    "    [D] Types of Machine Learning Papers\n",
    "    [D] The machine learning community has a toxicity problem\n",
    "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
    "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
    "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. Top 10 Posts of This Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>Author</th>\n",
       "      <th>Upvote Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[P] Finetuned Diffusion: multiple fine-tuned S...</td>\n",
       "      <td>Illustrious_Row_9971</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[D] DALL·E to be made available as API, OpenAI...</td>\n",
       "      <td>TiredOldCrow</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[P] Made a text generation model to extend sta...</td>\n",
       "      <td>Neat-Delivery4741</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[R] APPLE research: GAUDI — a neural architect...</td>\n",
       "      <td>SpatialComputing</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[P] Learn diffusion models with Hugging Face c...</td>\n",
       "      <td>lewtun</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[News] The Stack: 3 TB of permissively license...</td>\n",
       "      <td>Singularian2501</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[R] Reincarnating Reinforcement Learning (Neur...</td>\n",
       "      <td>smallest_meta_review</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[P] Transcribe any podcast episode in just 1 m...</td>\n",
       "      <td>thundergolfer</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[N] Adversarial Policies Beat Professional-Lev...</td>\n",
       "      <td>xutw21</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[P] Fine Tuning Stable Diffusion: Naruto Chara...</td>\n",
       "      <td>mippie_moe</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Post                Author  \\\n",
       "0  [P] Finetuned Diffusion: multiple fine-tuned S...  Illustrious_Row_9971   \n",
       "1  [D] DALL·E to be made available as API, OpenAI...          TiredOldCrow   \n",
       "2  [P] Made a text generation model to extend sta...     Neat-Delivery4741   \n",
       "3  [R] APPLE research: GAUDI — a neural architect...      SpatialComputing   \n",
       "4  [P] Learn diffusion models with Hugging Face c...                lewtun   \n",
       "5  [News] The Stack: 3 TB of permissively license...       Singularian2501   \n",
       "6  [R] Reincarnating Reinforcement Learning (Neur...  smallest_meta_review   \n",
       "7  [P] Transcribe any podcast episode in just 1 m...         thundergolfer   \n",
       "8  [N] Adversarial Policies Beat Professional-Lev...                xutw21   \n",
       "9  [P] Fine Tuning Stable Diffusion: Naruto Chara...            mippie_moe   \n",
       "\n",
       "   Upvote Score  \n",
       "0          1101  \n",
       "1           412  \n",
       "2           392  \n",
       "3           378  \n",
       "4           314  \n",
       "5           301  \n",
       "6           242  \n",
       "7           200  \n",
       "8           170  \n",
       "9           164  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# get top ten posts from this week (it's a generator)\n",
    "top_ten_gen = reddit.subreddit(f\"{ml_subreddit}\").top(time_filter=\"week\",limit=10)\n",
    "\n",
    "# init list for posts\n",
    "posts = []\n",
    "\n",
    "# get each post's title and author\n",
    "for post in top_ten_gen:\n",
    "    posts.append(\n",
    "        (post.title,post.author,post.score)\n",
    "    ) \n",
    "    \n",
    "# dataframe for a more neat display, including upvote score\n",
    "posts_df = pd.DataFrame(posts,index = [x for x in range(0,len(posts))], columns=[\"Post\",\"Author\",\"Upvote Score\"])\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details><summary>Expected output:</summary>\n",
    "\n",
    "    [N] Ian Goodfellow, Apple’s director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said “I believe strongly that more flexibility would have been the best policy for my team.” He was likely the company’s most cited ML expert.\n",
    "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
    "    [P] I’ve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple’s version of MobileNet, and more directly on your phone's camera roll.\n",
    "    [R] Meta is releasing a 175B parameter language model\n",
    "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
    "    [P] T-SNE to view and order your Spotify tracks\n",
    "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
    "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
    "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
    "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "Check out what other attributes the `praw.models.Submission` class has in the [docs](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html). \n",
    "\n",
    "1. After having a chance to look through the docs, is there any other information that you might want to extract? How might this additional data help you?\n",
    "*Answer_1: It would be helpful to add some other attributes such as number of comments, an upvote score, and whether the post was marked as a spoiler. Perhaps looking at the posts that included the spoiler attribute vs those without it would give some indication as to the factors that would affect a post's popularity.*\n",
    "\n",
    "\n",
    "Write a sample piece of code below extracting three additional pieces of information from the submission below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>Author</th>\n",
       "      <th>Upvote Score</th>\n",
       "      <th>Spoiler</th>\n",
       "      <th>Num of Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[P] Finetuned Diffusion: multiple fine-tuned S...</td>\n",
       "      <td>Illustrious_Row_9971</td>\n",
       "      <td>1102</td>\n",
       "      <td>False</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[D] DALL·E to be made available as API, OpenAI...</td>\n",
       "      <td>TiredOldCrow</td>\n",
       "      <td>412</td>\n",
       "      <td>False</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[P] Made a text generation model to extend sta...</td>\n",
       "      <td>Neat-Delivery4741</td>\n",
       "      <td>396</td>\n",
       "      <td>False</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[R] APPLE research: GAUDI — a neural architect...</td>\n",
       "      <td>SpatialComputing</td>\n",
       "      <td>380</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[P] Learn diffusion models with Hugging Face c...</td>\n",
       "      <td>lewtun</td>\n",
       "      <td>317</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[News] The Stack: 3 TB of permissively license...</td>\n",
       "      <td>Singularian2501</td>\n",
       "      <td>299</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[R] Reincarnating Reinforcement Learning (Neur...</td>\n",
       "      <td>smallest_meta_review</td>\n",
       "      <td>242</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[P] Transcribe any podcast episode in just 1 m...</td>\n",
       "      <td>thundergolfer</td>\n",
       "      <td>197</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[N] Adversarial Policies Beat Professional-Lev...</td>\n",
       "      <td>xutw21</td>\n",
       "      <td>165</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[P] Fine Tuning Stable Diffusion: Naruto Chara...</td>\n",
       "      <td>mippie_moe</td>\n",
       "      <td>160</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Post                Author  \\\n",
       "0  [P] Finetuned Diffusion: multiple fine-tuned S...  Illustrious_Row_9971   \n",
       "1  [D] DALL·E to be made available as API, OpenAI...          TiredOldCrow   \n",
       "2  [P] Made a text generation model to extend sta...     Neat-Delivery4741   \n",
       "3  [R] APPLE research: GAUDI — a neural architect...      SpatialComputing   \n",
       "4  [P] Learn diffusion models with Hugging Face c...                lewtun   \n",
       "5  [News] The Stack: 3 TB of permissively license...       Singularian2501   \n",
       "6  [R] Reincarnating Reinforcement Learning (Neur...  smallest_meta_review   \n",
       "7  [P] Transcribe any podcast episode in just 1 m...         thundergolfer   \n",
       "8  [N] Adversarial Policies Beat Professional-Lev...                xutw21   \n",
       "9  [P] Fine Tuning Stable Diffusion: Naruto Chara...            mippie_moe   \n",
       "\n",
       "   Upvote Score  Spoiler  Num of Comments  \n",
       "0          1102    False               56  \n",
       "1           412    False               59  \n",
       "2           396    False               55  \n",
       "3           380    False                7  \n",
       "4           317    False               14  \n",
       "5           299    False               32  \n",
       "6           242    False               31  \n",
       "7           197    False                9  \n",
       "8           165    False               49  \n",
       "9           160    False                8  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def get_top_ten(subreddit_name):\n",
    "    \n",
    "    # get top ten posts from this week (it's a generator)\n",
    "    top_ten_gen = reddit.subreddit(f\"{subreddit_name}\").top(time_filter=\"week\",limit=10)\n",
    "\n",
    "    # init list for posts\n",
    "    posts = []\n",
    "\n",
    "    # get each post's title and author\n",
    "    for post in top_ten_gen:\n",
    "        posts.append(\n",
    "            (\n",
    "                post.title,\n",
    "                post.author,\n",
    "                post.score,\n",
    "                post.spoiler,\n",
    "                post.num_comments\n",
    "            )\n",
    "        ) \n",
    "\n",
    "    # return dataframe for a more neat display, including upvote score\n",
    "    posts_df = pd.DataFrame(posts,index = [x for x in range(0,len(posts))], columns=[\"Post\",\"Author\",\"Upvote Score\",\"Spoiler\",\"Num of Comments\"])\n",
    "    posts_df\n",
    "    \n",
    "    return posts_df\n",
    "\n",
    "get_top_ten(ml_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "2. Is there any information available that might be a concern when it comes to Ethical Data?\n",
    "*A2: Not really in my opinion. There is not any personal information, location information, or sensitive information available from this API from what I have seen so far. There might be an ethical question of using a reddit user's name, but I don't see any issues that could be questionable from an ethical perspective.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. Comment Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "\n",
    "The purpose is \n",
    "1. to understand what the code is doing \n",
    "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later 😊) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n",
      "gh1dj9\n",
      "kuc6tz\n",
      "[Submission(id='gh1dj9'), Submission(id='kuc6tz')]\n",
      "CPU times: user 75 ms, sys: 11.9 ms, total: 86.9 ms\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from praw.models import MoreComments\n",
    "\n",
    "print(subreddit.title)\n",
    "\n",
    "# store top comments in a list\n",
    "top_comments = []\n",
    "\n",
    "# iterate over each submission in an iterator of top 2 posts\n",
    "for submission in subreddit.top(limit=2):\n",
    "    print(submission)\n",
    "    \n",
    "    top_comments.append(submission)\n",
    "    \n",
    "#    for each top level comment\n",
    "    for top_level_comment in submission.comments:\n",
    "\n",
    "        # check if the top level comment has even more comments on it\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        \n",
    "        # continue to the next comment...\n",
    "        # save each comment to the top comment when\n",
    "            top_comments.append(top_level_comment.body)\n",
    "            \n",
    "        \n",
    "print(top_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. Inspect Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many comments did you extract from the last step? Examine a few comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter thread: [https://twitter.com/cyrildiagne/status/1259441154606669824](https://twitter.com/cyrildiagne/status/1259441154606669824)\n",
      "\n",
      "Code: [https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard](https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard)\n",
      "\n",
      "Background removal is done with U^(2-Net) (Qin et Al, Pattern Recognition 2020): [https://github.com/NathanUA/U-2-Net](https://github.com/NathanUA/U-2-Net)\n",
      "\n",
      "**/!\\\\ EDIT:** You can now subscribe to a beta program to get early access to the app: [https://arcopypaste.app](https://arcopypaste.app)  !\n",
      "Simple yet very useful. Thank you for sharing the code.\n",
      "The future 🤯\n",
      "Ohh the nightmare of making this into a stable product... Enough to drive you mad just thinking about it\n",
      "Almost guaranteed, Apple will copy your idea in 3, 2, 1....\n",
      "Wtffff. Well that was incredible.\n",
      "Apple can’t wait to steal this and not credit the creators\n",
      "fantastic!\n",
      "Why did the boxes in the diagram turn gray?\n",
      "How does the Algorithm decide what it cuts out from the input pictures? \n",
      "\n",
      "For example it only cut out the two people in the picture and not the surroundings.\n",
      "\n",
      "Amazing project though!\n",
      "#WITCH!  BURN THEM!\n",
      "This will be amazing if released, even as a beta. Definitely can see this being very useful\n",
      "Any sufficiently advanced technology is indistinguishable from magic.\n",
      "Really good work, thanks for sharing!\n",
      "I'm extremely impressed with it cutting dark hair from a brown background. Is that the pixel's camera doing the hard work or is it U^2_Net ? Have you tried it with other phones? How does it deal with feathering? Stunning demo & thanks for posting this.\n",
      "Super cool\n",
      "Wizardry!\n",
      "Woahhh that is so cool!!! I am wondering the speed wise from the initial snap till pasting it to computer.\n",
      "\n",
      "If we could get it done >1s I think this project would be really fun and useful. Allow me to fork the project ;)\n",
      "\n",
      "Thank youuu\n",
      "This is God like!\n",
      "Wow. What you did wlth AR is really creative and very impressive technically. Keep going dude you rock.\n",
      "Holy fucking shit my jaw hasn’t dropped like this since I saw the GPT-2 demo. This is absolutely unreal—it is so precise + how the hell do they interact with macOS like that? Wow. Awesome work pal, so much respect.\n",
      "Super cool demo.\n",
      "\n",
      "But the more interesting part to me is the app actually look at the computer screen to decide what target the image/content is pasted to. \n",
      "\n",
      "Probably hard-coded, but super interesting idea.\n",
      "This is amazing. Congratulations!!!\n",
      "This is amazing! Thanks for sharing the code\n",
      "Awesome! Recognize the catalog from Coder le Monde\n",
      "That is so cool!\n",
      "Awesome, will try it definitely.\n",
      "Take my money\n",
      "God this, and swiping a window to my laptop from my phone with a simple gesture, is what I have been waiting for sooo long.\n",
      "cyberpunk level shit\n",
      "Wow. Thanks for sharing!\n",
      "This is really well done. From research to a simple yet useful use case!\n",
      "beautiful\n",
      "This is brilliant. Thanks for sharing...\n",
      "Say sike 🤯🤯\n",
      "Whoah\n",
      "I saw this the other day and I thought it was incredible. I'm a novice on programming but ill do my best to deploy this on my PC just to play around with it! Thanks a lot for sharing this with the world!\n",
      "Smart move\n",
      "That's some next level copy -paste !\n",
      "This is so cool! AI never ceases to amaze me.\n",
      "10 years ago people would laugh at this idea.\n",
      "Wow this is so helpful, insane\n",
      "This is so crazy!\n",
      "So good it looks fake af\n",
      "This is probably the coolest thing I have seen in a long while.  Great fucking work!\n",
      "Wow, this is sick\n",
      "You sir are a genius\n",
      "What are the edges cases when this doesn't work? Does this require certain lighting conditions etc? How does it know to extract both people from the image?\n",
      "Very impressive, thought it was fake at first.... 🤪\n",
      "Amazing!\n",
      "Wow.\n",
      "This gets 100 very nices\n",
      "This is insane.\n",
      "Man, this is awesome!\n",
      "This is amazing. If you have any intention of publishing this as an end user app, hit me up, I’ll get make sure you get sponsorship for all the GPUs and other compute you need.\n",
      "This is brilliant!\n",
      "This is some crazy Tony stark shit\n",
      "This is something really superb!!!!!!!!!!!  \n",
      "I loved the technology...\n",
      "\n",
      "AI and Machine learnings are actually contributing a lot in streamlining our daily processes. I mean, this is something, being a student I would need the most, instead of first emailing myself pictures from phone, then downloading them and inserting them in my doc.\n",
      "Woke up in the morning and this is the first thing I see. A day can’t get more inspirational.  I can’t thank you enough for sharing.\n",
      "Wow, this is epic!\n",
      "What is difference between this and taking photo and sending it with email to computer? 🤔 What is the main use case for this technology?\n",
      "I really hope your idea doesn't get stolen. Also how do I keep up to date with your progress?\n",
      "Did you train the ML model yourself? If so what data set did you use?\n",
      "How were you able to get integration with chrome and slides itself? Are you able to load custom software through Google Slides somehow?\n",
      "How do I do this?\n",
      "This is so cool! Is it really necessary to point the phone at the screen to paste it?  Or will it just paste it into whatever application is currently focused no matter what?\n",
      "Does it work with text as well?\n",
      "u/fabiomb el otro día decías que andaba porque tenía fondo de color blanco plano.\n",
      "I'm more impressed with the background extraction on the photo than with the multidevice \"copy-paste\"\n",
      "u/vRedditDownloader\n",
      "u/VeedditDownloader\n",
      "u/vredditdownloader\n",
      "That's so cool\n",
      "There's no way that took 10s to develop, install, try and record an 57 sec video of. I mean, yeah, technology and stuff, but not in 10s. Sorry.\n",
      "I will go through the damn code line by line!\n",
      "Is that a Google Pixel?\n",
      "I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:\n",
      "\n",
      "- [/r/mattslinks] [Augmented reality cut n paste](https://www.reddit.com/r/mattslinks/comments/ht7vs5/augmented_reality_cut_n_paste/)\n",
      "\n",
      "&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*\n",
      "This is beast!! Deff on to something!\n",
      "Awesome.\n",
      "How do we use this\n",
      "more more..More..MOREEEEEE\n",
      "Dude...love ur copy paste...\n",
      "Daaaaaaaam!\n",
      "WITCH!\n",
      "What the fuuuuuuuuck?\n",
      "photoshop required ?\n",
      "Omg! Thats awesome!!! I had a similar idea but using text\n",
      "u/vredditdownloader\n",
      "This is beyond science\n",
      "My 5 years daughter thought of something similar..for her she wants that you take the object out of the screen and you show its hologram presentation..she said that would be a hard project to achieve :))\n",
      "I will show her your project tomorrow, she will like it.\n",
      "Oh sure. I find this after spending 29 days scanning in 21 years of issues of an instructional magazine on a flat bed scanner\n",
      "[deleted]\n",
      "Really impressive if it works as well with unseen data.\n",
      "\n",
      "Still fun if it doesn't.\n",
      "This is clearly fake....the last screen shot proves it.\n",
      "In 3 seconds if you use anything else then MacBook 😏\n",
      "Ok, that’s the coolest thing I’ve seen in a long while.\n",
      "This is awesome.\n",
      "Tony Stark shit\n",
      "Good job\n",
      "Let we know when there's an easy and seamless way of doing this, or at least no-brainer\n",
      "Wow dude, I don't know shit about ML, all I can say is this is superpower\n",
      "I need this for editing for my small business. No more Adobe illustrator.\n",
      "fuck capitalism\n",
      "\n",
      "\n",
      "wreak havoc on the middle class\n",
      "If it wasn't for the link with the code, I would have straight up thought you were trying to trick us :o\n",
      "This is insaneeee! I have a question about it's applications:\n",
      "\n",
      "is it possible to use this to extract information? For example if you scan a receipt could it create a digital version with each line editable?\n",
      "Holy smokes this is insanely awesome. Thank you\n",
      "Took words out of my mouth\n",
      "I mean, technically now it's the past.\n",
      "Why would it be a nightmare?\n",
      "His license even allows commercial use, so they are legally allowed to do that\n",
      "Lol and he's using a pixel too\n",
      "Likely, and it will be easier for them, the processing could be done in the iPhone and uploading can be done through airdrop (which supports 'aiming' at people and machines to share files).\n",
      "[deleted]\n",
      "They'll probably slap a patent on it too and sue the original creators.\n",
      "> Apple can’t wait to steal this and not ~~credit~~ **sue** the creators\n",
      "\n",
      "ftfy\n",
      "Already stolen and and implemented into the next iOS...\n",
      "Thanks!\n",
      "U^2-Net decided not to remove the background of these :)\n",
      "Check out the details for U^2-Net on the official repo: https://github.com/NathanUA/U-2-Net\n",
      ">, even as a beta\n",
      "\n",
      "The code is available, so you can play already with it :)   \n",
      "[https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard](https://github.com/cyrildiagne/ar-cutpaste/tree/clipboard)\n",
      "and any sufficiently understood magic is indistinguishable from technology.\n",
      "It is 100% handled by U^2-Net: Check out the official repo for more information and samples: https://github.com/NathanUA/U-2-Net\n",
      "Yep maybe add a ghost non-transparent-background\n",
      "to make up for the delay of the BG removal.\n",
      "\n",
      "I'm just impressed by the copy paste AR stuff. Well done!\n",
      "Hi! The coordinates are automatically defined by the receiving software in this demo but checkout my precious demo where I use OpenCV SIFT to find the correct coordinates on the screen\n",
      "Please checkout the official U^2-Net repo for more information on the background substraction: https://github.com/NathanUA/U-2-Net\n",
      "Edge cases mostly are busy scenes when there are no particular salient element\n",
      "It just save time and headaches but the result is identical\n",
      "\n",
      "Although you get background removal for free in the process ;)\n",
      "Looks like it also recognizes the photo subject to only copy the link important bits. Also faster\n",
      "Thanks! For now the most updated news are on my Twitter!\n",
      "I'm using the pretrained model from U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection, Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R. Zaiane and Martin Jagersand: https://github.com/NathanUA/U-2-Net\n",
      "Juste the clipboard and pyautogui to send the \"paste\" keystrokes :)\n",
      "Checkout the repository!\n",
      "Good point! For now the code only paste at whatever app is active. In some apps (like Photoshop) you can paste at specific coordinates depending on where you point the phone\n",
      "Not yet ;)\n",
      "Ahí me gustó más 👍\n",
      "*beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!\n",
      "\n",
      "* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)\n",
      "\n",
      "* [Audio only](https://v.redd.it/v492uoheuxx41/audio)\n",
      "\n",
      "I also work with links sent by PM\n",
      "\n",
      " ***  \n",
      "[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;❤**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader)\n",
      "*beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!\n",
      "\n",
      "* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)\n",
      "\n",
      "* [Audio only](https://v.redd.it/v492uoheuxx41/audio)\n",
      "\n",
      "I also work with links sent by PM\n",
      "\n",
      " ***  \n",
      "[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;❤**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader)\n",
      "have you had a chance to do that? has anyone tried running that code on their device?\n",
      "Yes! But it also work with iphones\n",
      "*beep. boop.* I'm a bot that provides downloadable links for v.redd.it videos!\n",
      "\n",
      "* [**Download** via https://reddit.tube](https://reddit.tube/d/1vfDX9G)\n",
      "\n",
      "* [Audio only](https://v.redd.it/v492uoheuxx41/audio)\n",
      "\n",
      "I also work with links sent by PM\n",
      "\n",
      " ***  \n",
      "[**Info**](https://old.reddit.com/user/VredditDownloader/comments/cju1dg/info/)&#32;|&#32;[**Support&#32;me&#32;❤**](https://www.paypal.me/synapsensalat)&#32;|&#32;[**Github**](https://github.com/JohannesPertl/vreddit-downloader)\n",
      "Hmm, I don't think the 2nd image with the 2 persons could have be done with OpenCV?\n",
      "What do you mean? The service runs remotely and it has never seen the images used in the video\n",
      "I guarantee it's not fake.\n",
      "Is the ELI5 version is this? - \n",
      "\n",
      "1. The React Native application on the mobile takes a pictures\n",
      "2. The picture get uploaded to the server where the background is removed with u2-net, which is the brains of the operation\n",
      "3. The removed background is then communicated through a python script > Photoshop plugin to be added on to a running project or is saved as a transparent image.\n",
      "\n",
      "Right?\n",
      "Yes. But you will need to a run OCR instead of the background removing application (U\\^2-Net)  OP is using.\n",
      "This would be the killer use case.\n",
      "How can it be the past if its happening now?... id say the now its the future and past overlapping for a jiffy of a second\n",
      "Well, why don't we already have cross-device AR interfaces and can swipe content between our devices seamlessly like Tony Stark? The U-net demonstrated here only provides a means to extract relevant sections of image data. The rest that needs to be done for this demo is far more difficult. Roughly, you need to identify the other device through the camera or NFC, pinpoint the relative position of the two devices for the onscreen insertion position, match the other device to a Bluetooth device or wifi connected device securely, set up a transfer, communicat data type and decide what should happen with the data... and do all of this across different OS and devices with different standards, handle poor connection, communicate all the issues to the user in a foolproof way. You can force the user to setup some of this manually, but then you'll loose 99% of the users and the product won't gain enough support/funding and gets dropped like a pair of Google glasses.\n",
      "[deleted]\n",
      "Wait. I thought they'll sew my ass to the mouth of another person who accepted the ToS.\n",
      "But capitalism drives innovation by rewarding innovators. That's why we have all the smart humanitarian millionaires pushing humanity towards brighter future.\n",
      "\n",
      "/s\n",
      "I put up a public predictor API endpoint for the ML model so you don't have to battle with GPUs when playing with this. Simply start the server with `--basnet_service_ip http://basnet-predictor.tenant-compass.global.coreweave.com/` and that piece is taken care off.\n",
      "Beautiful thanks! I'm excited to try this.\n",
      "How long does inference take generally? Is your video realtime? Because it's surpringly fast for an HD photo from a phone.\n",
      "How do you handle the domain shift between digital images and photos of images captured with a camera?\n",
      "\n",
      "(i.e. perspective, glare, curvature, lighting)  \n",
      "Or do you just hope the pretrained network generalizes well enough?\n",
      "I see what you are talking about. Yess, definitely can do that!! OP is a badass\n",
      "If it removes the background you want.\n",
      "Oh...true. I was overthinking it haha\n",
      "RemindMe! 4 days\n",
      "Cool! I always assume the examples used in presentations are part of the training data unless told otherwise.\n",
      "\n",
      "~~From a quick look at the code, I guess it's based on this paper?~~  [~~http://openaccess.thecvf.com/content\\_CVPR\\_2019/papers/Qin\\_BASNet\\_Boundary-Aware\\_Salient\\_Object\\_Detection\\_CVPR\\_2019\\_paper.pdf~~](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Nevermind, the description on Github answers that question, was just to lazy to read it before jumping into the code :P\n",
      "Awesome! thanks for the reply! \n",
      "\n",
      "Completely understand if you're not able offer this, but do you know of any good, reliable OCR services?\n",
      "\n",
      "All the ones I've found, tend to be geared towards a particular functionality and after testing several they just don't live up to the standard I'm expecting\n",
      "No it wouldn't, because it already exists in dozens of apps. Including Google Lens, installed on hundreds of millions of phones.\n",
      "Future, past, now is simply an illusion. Reality is one continuum, it can't be neatly divided into division as such. But, this kind of conceptualization might have tremendous utility in our daily life.\n",
      "Yeah man and this all those companies fault who use different standard for every fucking thing (microsoft and apple looking at u) .\n",
      "You just need to run some visualbasic to check the camera on the phone to know where on the pc it's pointing to, then send that image to the pc with the coords to paste the amethyst. I can do this with one weekend.\n",
      "Why bother patenting it? Does he have the money to patent it, does he have the money to protect the patent against apple or google, can it be patented, does he have the intellectual resources to patent...\n",
      "\n",
      "Patents are for companies to monetise their R&D, not for individuals to get rich. This dude would probably be happy enough getting some corporate credibility and could potentially lead a team if google or apple are interested in this.\n",
      "Yeah ok, we'll wait here to hear the news in 3 years about whether or not it got approved.\n",
      "What system does drive innovation then? Do you wanna say that socialism/communism pushed their country towards brighter future?\n",
      "Video is real-time but inference is don't on a 320x320 image. But that's only the resolution of the alpha mask , the image can have native reslution\n",
      "I will be messaging you in 4 days on [**2020-05-15 12:56:47 UTC**](http://www.wolframalpha.com/input/?i=2020-05-15%2012:56:47%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/gh1dj9/project_from_books_to_presentations_in_10s_with/fq9lu04/?context=3)\n",
      "\n",
      "[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Fgh1dj9%2Fproject_from_books_to_presentations_in_10s_with%2Ffq9lu04%2F%5D%0A%0ARemindMe%21%202020-05-15%2012%3A56%3A47%20UTC) to send a PM to also be reminded and to reduce spam.\n",
      "\n",
      "^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20gh1dj9)\n",
      "\n",
      "*****\n",
      "\n",
      "|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n",
      "|-|-|-|-|\n",
      "Have you tried tesseract? One year back i tried it with a little project, it work quite well out of the box.\n",
      "[deleted]\n",
      "Where can I read more about this ?\n",
      "https://xkcd.com/927/\n",
      "Sure! Go for it buddy.\n",
      "I think if workers were to own their workplaces, instead of stock holders, we would be living in better world. In effect that means you can only own a piece of company if you work there.\n",
      "\n",
      "This is theory, I don't think this has ever been tried in any existing society. There are some worker-cooperatives but they have to compete with capitalist companies which do not have the moral restrictions a worker owned company have, so they are naturally at a disadvantaged position.\n",
      "no never heard of it! kicking myself now that I wasn't able to find it whilst looking for a solution.\n",
      "\n",
      "This looks promising so thank you :) I'm a designer with some programming knowledge myself so I imagine it'll take a while to really test it out for my purpose.\n",
      "\n",
      "Thanks so much again!\n",
      "Makes sense!\n",
      "Idk I cooked that up [I'm a wannabe philosopher ;)] but you can find many parallels in some Philosophers' works like Neitzsche's Cause and effect theory.\n",
      "\n",
      "I must add, both Space and Time are wholly inference based derivatives, we can't or haven't perceived them. All the Space-time continuum metaphysical talks are purely theoretical. No experiment have given emperical evidence of them.\n",
      "Wow never thought like that cheers !!!\n",
      "dId He dO It\n",
      "So worker-owned companies have been tried and failed, but it's capitalism's fault?\n",
      "\n",
      "Actually, how do these things even work? Who makes the decisions in the absence of clear owner? What constitutes a \"worker\"? How are the company shares divided between workers - evenly, or according to their positions?\n",
      "Just to be clear: Apple is a company that was owned by its workers initially. Those workers decided to sell a piece of it to investors early on because they (the workers) decided it would help them grow faster. Then they were successful, and they (the workers) decided to sell more of the company so they could buy nice houses and make charitable contributions. Apple *is* the result of a worker-owned-company system, albeit one that gave those workers the freedom to sell their stock for various reasons along the way.\n",
      "Thank you. Sounds fascinating!\n",
      ">So worker-owned companies have been tried and failed, but it's capitalism's fault?\n",
      "\n",
      "That's like saying \"not stealing has been tried but it was less profitable to stealing, so now you are blaming thieves that the losers who don't steal lost?\"\n",
      "\n",
      ">Actually, how do these things even work? \n",
      "\n",
      "You can research these topics yourself. For example /r/Anarchy101 has smarter people than me to explain these topics.\n",
      "That's interesting. Didn't know that. But Apple was worker-owned company until it turned into capitalist company. When they hired their first employee who didn't own a piece of that company, they made an ideological choice to exclude that employee from the profits the company produced and in effect created hierarchy inside their company.\n",
      ">\"not stealing has been tried but it was less profitable to stealing, so now you are blaming thieves that the losers who don't steal lost?\"\n",
      "\n",
      "The reason not to steal is because there are deliberate mechanisms in place to dissuade, not because it's a less efficient way to make money. In contrast, socialism *is* less efficient than capitalism.\n",
      "\n",
      ">You can research these topics yourself.\n",
      "\n",
      "Why is it that every single anti-capitalist assigns homework when poked with a stick? Consider that you are arguing in favor of uprooting industry as a whole and can't even articulate why.\n",
      "\n",
      ">This is theory, I don't think this has ever been tried in any existing society.\n",
      "\n",
      "This is another favorite. You are betting the farm -- hell, *society* -- on a theory. On something that has not even been validated. Doesn't that seem a little bonkers to you?\n",
      "Actually, in most countries stealing is less profitable than any legitimate way of earning money thanks to law enforcement. So in the same vein, it's USA government's fault for letting capitalism go unchecked there, but no fault of the system itself.\n",
      "I suspect for much of Apple’s life, the vast majority of Apple employees owned a piece of the company. I don’t know if Apple Store employees do - they may not - but Apple stores are a comparatively recent addition and I bet the engineering staff through the 90s were employee-owners as that’s the norm in Silicon Valley. At some point Apple made a decision to contract out manufacturing, so the people actually building Apple computers and phones are mostly not Apple employees and not owners. And it was certainly a significant decision to bring on venture capital investors (and later public shareholders) who were not employees of the company (though I think history would show pretty clearly that if they hadn’t done that we wouldn’t have Apple today). \n",
      "\n",
      "But I do think it’s worth noting that all of these *decisions* were, at Apple, mostly made by the early employees / founders, not by third party shareholders. That’s idiosyncratic to tech, an industry dominated by strong founders, but it’s true at Apple, at Google, at Facebook, at Amazon, etc - the decision makers are the founding employees. In fact at many of these companies the founders have put in place systems such that “capitalist” public owners explicitly DON’T have control of the business, only the founders do, long after the founder ownership levels have decreased.\n",
      "\n",
      "Even Goldman Sachs was a “partnership” for most of its history - ENTIRELY owned by a subset of its employee population. This is true for every major large corporate law firm today. Just because these businesses are “employee owned” clearly doesn’t mean they’re run “for the benefit of the people” - they’re run by rich early employees who want to get richer (and maybe have other motivations, like building great products, or personal celebrity, or whatever). \n",
      "\n",
      "I am a huge fan of “employee equity ownership” - most startups are built on the back of this idea - though most employees in turn eventually want to be able to sell their shares to other people (so they can buy houses and cars, or make charitable contributions or whatever). But I’m not sure employee ownership is a radical departure from ‘capitalism’ as you describe it - the ends ultimately look pretty similar to companies that are not employee owned.\n",
      "> Why is it that every single anti-capitalist assigns homework when poked with a stick? \n",
      "\n",
      "Because I'm not your teacher. You look up this stuff yourself if you are interested.\n",
      "\n",
      ">You are betting the farm -- hell, society -- on a theory. On something that has not even been validated. Doesn't that seem a little bonkers to you?\n",
      "\n",
      "That's the dilemma of sociology in general. You can't run controlled experiments without affecting human lives and you can't remove yourself from the equation and be a neutral observer because you are part of that society.\n",
      "> Because I'm not your teacher. You look up this stuff yourself if you are interested.\n",
      "\n",
      "I am interested, and I have tried to look it up. I'm pointing out that it's not a compelling argument to say \"go look it up\" when you are trying to change the status quo.\n",
      "\n",
      "It doesn't matter whether you convince me or not; I'm not here to be convinced, and you aren't here to convince me. The problem is that *no one* can seem to articulate why we should be socialist. It always ends up as a homework assignment no matter who I talk to. \n",
      "\n",
      ">That's the dilemma of sociology in general.\n",
      "\n",
      "I agree. \n",
      "\n",
      "Fortunately for capitalists (and unfortunately for you), capitalism isn't the outcome of a controlled experiment. Rather, it is [*emergent*](https://en.wikipedia.org/wiki/Emergence). The free market exists as a result of every individual acting according to his or her individual incentives, not because a committee decided that this is the way we should do it.\n",
      "\n",
      "So not only is the statement that we should uproot the entire economic system incredibly arrogant, since it is predicated on the assumption that you will implement it properly (in the context of the incentives that exist for the people implementing it), proving that it will work at all requires evidence that cannot be obtained. As you noted, there is no way to conduct a controlled experiment in this area, so we are kind of stuck with what we've got.\n",
      "\n",
      "[Here](https://en.wikipedia.org/wiki/Holodomor) is what happens when you get too clever.\n",
      "The fact that they also had to know the location of the numbers and that the algorithm was robust to scale changes is impressive for 1993\n",
      "\n",
      "It's not like they just solved MNIST in 1993, it's one step above that\n",
      "Every data scientist today is truly standing on the shoulders of giants.\n",
      "awesome to see\n",
      "TIL audio hasn’t been invented until 1994\n",
      "Anyone know who the other guys at the end are?\n",
      "And yet websites still think those obfuscated texts are a good test for robots\n",
      "Man, these guys were the real engineers.\n",
      "Never going to complain about not having a strong enough GPU again. Very cool.\n",
      "Actually, he was 32 years old when he pressed the button. He was 33 by the time he got the results back.\n",
      "Wonder what was the RAM and computing power of the system.\n",
      "Many don’t know it, but before it was done such text recognition was considered impossible, just like AGI and other hard problems. I think text recognition in mail was the first successful real world application of AI.\n",
      "MNIST irl\n",
      "that was certainly more wholesome than the other historic computer vision video, [https://www.youtube.com/watch?v=8VdFf3egwfg](https://www.youtube.com/watch?v=8VdFf3egwfg)\n",
      "But the question is: is it the validation set? 😁\n",
      "Very inspiring as I remember these days. Lot of hard work and at the cutting edge.\n",
      "Uh.  Sorry, no.\n",
      "\n",
      "[The CNN was invented by Hubel and Weisel in 1959, the year before Yann LeCun was born, under the name \"neocognitron.\"](https://en.wikipedia.org/wiki/Neocognitron) \n",
      "\n",
      "LeCun also didn't make them first.\n",
      "\n",
      "[The CNN was first implemented by Kunihiko Fukushima in 1979](https://search.ieice.org/bin/summary.php?id=j62-a_10_658), 14 years before this video\n",
      "\n",
      "(Reference translated is Journal of the Institute of Electronics, Information and Communication Engineers A Vol.J62-A No.10 pp.658-665, October 25, 1979, ISSN 0373-6091)\n",
      "\n",
      "What Yann LeCun actually brought to the party was the modern approach to training them.  He did that in 1984, not 1993.\n",
      "Nice keeb.\n",
      "u/savevideo\n",
      "That is so satisfying\n",
      "The first set of numbers was Yann LeCun's phone number at bell labs.\n",
      "Still accurate than tesseract lol 😂\n",
      "So why am I still doing captchas\n",
      "Yann LeCun's tweet on who the other guys are, and who the cameraman is - \n",
      "https://twitter.com/ylecun/status/1347268914263306242?s=20\n",
      "Better than tesseract\n",
      "But still, to this date, they cannot recognize traffic lights\n",
      "incredible! pay tribute to him\n",
      "So why did it take 30 years to get this far?\n",
      "On some comments about possible tweaks/tricks in this video:\n",
      "\n",
      "I have had the privilege to attend professor Yann's classes at NYU. \n",
      "From whatever little I understand of him - he has high levels of integrity, and I do not see him trying some cheap tweaks and fixes...He was committed to solve a problem in the best way possible and not just for likes and hearts ☺️.\n",
      "\n",
      "And without high level of integrity, you can't go from lab to national level in short time. \n",
      " \n",
      "People often underestimate what it takes to be unanimously accepted as one of the godfathers of current hottest trend. This doesn't discount the effort of forefathers or future generations...\n",
      "... but let's not undermine Prof's integrity and commitment by making such frivolous comments. In fact, it is only our loss, if we fail to see that.\n",
      "Cant see his right hand\n",
      "Outside of the CNN achievements the rest is actually impressive too, and I'm absolutely amazed that the interface is so responsive. In 1993.\n",
      "I'll never understand why this didn't blow up like it should have when they succeeded in doing this. Should've been in the news all over the place for months.\n",
      "\n",
      "AI winter my backside\n",
      "What a boss!\n",
      "Fukushima’s neocognitron came almost two decades earlier.\n",
      "[deleted]\n",
      "So then what took so long for it to catch on? Why did it take another 30 years if they knew the power of cnn's?\n",
      "Amazing!  I’ve cited Professor LeCunn multiple times and am always humbled by his work — this is why I tell students that they are standing on the shoulders of giants when they do research.  Love this video!!!\n",
      "Are you sure you’re a robot?\n",
      "I guess too many people underestimate what could be accomplished with a little and tons of passion and time\n",
      "Agree - it was 6 years later until MNIST was even released.\n",
      "I guess they had a preprocessing step to identify, center and scale each digit image before feeding into the neural network. It’s not that hard with feature engineering.\n",
      "The video has lots of cuts, and the numbers never obscures an important part of the image...    I suspect each of those tests had tweaking and tuning to make it work...\n",
      "Love how happy they look!\n",
      "I was born 1982. We didn't start hearing shit until 1995. That was an absolutely wild year. It created a real musical renaissance.\n",
      "Can confirm.  That's the year I got a sound card.\n",
      "Am son of the guy in the chair (Rich Howard, collaborator and director of the silicon integrated circuit lab at the time). He said the guy in orange was a technician and computer whiz named Donnie Henderson.\n",
      "there is a reason why captcha is becoming obsolete. At least the text based version.\n",
      "\n",
      "Also, captcha actually digitize books. This is why there are 2 tests, not 1. So in a sense, we were training the robots filling the captchas.\n",
      "It serves two purposes. It defeats 99.99% of bots, and it maps images to human inputs to train their image recognizer networks.\n",
      "I don’t think it’s meant to filter that way. Bots usually are built with speed in mind so it recognises and fills in the blanks virtually immediately.\n",
      "\n",
      "That and captchas are also useful for labelling training datasets manually (user input). But correct me if I’m wrong though.\n",
      "Unless someone cares enough about your little website to train an AI to solve your captcha they're still not a terrible idea. I don't think there are any AIs that are generic enough to solve *all* obfuscated text captchas yet.\n",
      "\n",
      "Obviously it's not going to work for large sites but none of them use that method anymore anyway.\n",
      "err, Kurzweil had an OCR product in 1976: [https://en.wikipedia.org/wiki/Ray\\_Kurzweil#Mid-life](https://en.wikipedia.org/wiki/Ray_Kurzweil#Mid-life)\n",
      "Bayesian classifiers as the first email spam filter?\n",
      "\n",
      "Not sure the year, but our lives would be completely different if it wasn’t for it.\n",
      "> Many don’t know it, but before it was done such text recognition was considered impossible\n",
      "\n",
      "By the time LeCun did this, text recognition was common at banks for scanning checks, in children's toys, and was the basis of the Cue:CAT.\n",
      "\n",
      "You're making this up.\n",
      "\n",
      "OCR was common by the early 1970s, almost 30 years before this.\n",
      "No\n",
      "You are NOT correct about Hubel and Weisel.\n",
      "\n",
      "Hubel and Weisel did research on visual cortex in real brains (in cats) and it was awesome (they got Nobel Prize for it). But they did not invent CNNs.\n",
      "\n",
      "You can read their paper \\[1\\] you don't have to be a biologists to understand most of it. From their work one can deduce what neurons in V1 do. It was later even verified that some of these neurons realize functions similar to Gabor filters, but (as I remember) that was even later then neocognitron.\n",
      "\n",
      "It is true that their findings did *inspire* creators of neocognitron \\[2\\] but that's about it.\n",
      "\n",
      "\\[1\\] [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf)\n",
      "\n",
      "\\[2\\] Fukushima, Kunihiko, and Sei Miyake. \"Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition.\" *Competition and cooperation in neural nets*. Springer, Berlin, Heidelberg, 1982. 267-285.\n",
      "And to add to this, people thought NN's were a joke until a CNN won an image recognition contest in 2012, which is what put them on the map.  Before that they were obscure and overlooked.\n",
      "###[View link](https://redditsave.com/info?url=/r/MachineLearning/comments/kuc6tz/d_a_demo_from_1993_of_32yearold_yann_lecun/)\n",
      "\n",
      "\n",
      " --- \n",
      " [**Info**](https://np.reddit.com/user/SaveVideo/comments/jv323v/info/)&#32;|&#32; [**Feedback**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Feedback for savevideo) &#32;|&#32; [**DMCA**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Content removal request for savevideo)&#32;|&#32;[**Donate**](https://ko-fi.com/getvideo)\n",
      "at least, read the title\n",
      "Yes. Back then, the proportion of developers who could hand-write a new graphics algorithm in assembler or C was considerably higher, since that was often how it was done anyway. Necessity is a great motivator. The non-ML part of this problem is more tedious than difficult.\n",
      "Maybe I’m misunderstanding, but isn’t the whole point of CNNs that the location of the digits doesn’t matter?\n",
      "This system ended up deployed in banks to parse written checks, so I don't think it was tweaked just for these examples, but they did expect to have fully visible digits.\n",
      "I have had the privilege to attend professor Yann's classes at NYU. \n",
      "From whatever little I understand of him - he has high levels of integrity, and I do not see him trying some cheap tweaks and fixes...He was committed to solve a problem in the best way possible and not just for likes and hearts ☺️.\n",
      "\n",
      "And without that level of integrity, you can't go from lab to national level in short time.\n",
      "[removed]\n",
      "I remember when we got sound in school for the first time there was alot of realization of where smells were actually coming from that day\n",
      "Imagine if The Bends was the first sound you ever heard.\n",
      "Yeah, well I was born in 72 and we ate rocks for breakfast!\n",
      "Lies and slander, PC speaker was readily available on PC before soundcards became a thing.\n",
      "That's super cool lol. Did this invention have a big impact on their career?\n",
      "Yeah, on a PHPBB forum I manage, the bots can get through the text-based captchas very easily. But they still struggle with simple questions like \"In what State is this club based?\"\n",
      "Not anymore. Google stopped doing that a while ago\n",
      "As I make my living making bots and doing automation, captcha is just part of the job. Solving captcha isn’t a special thing.\n",
      "You’re definitely correct about the captchas.  \n",
      "It’s no coincidence that most of the objects they ask you to recognize are cars, crosswalks etc.  \n",
      "They basically get free labor to help them build a giant dataset fir training self driving cars.\n",
      "what would be the problem with a little delay?\n",
      "“I’m not a robot” - select crosswalk, identify license plates, etc. are for training self driving vehicles and finding the house address was for google maps. \n",
      "\n",
      "We should be paid for doing reCaptchas. However some people actually do get paid for these tasks.\n",
      "Correct me if I'm wrong, but doesn't normal font imply a \"set font\" rather than handwritten characters?\n",
      "\n",
      "Still impressive but a different problem from MNIST and generally reading the messy writing of humans.\n",
      "But Schmidhuber had already written the paper in 1962\n",
      "> Bayesian classifiers as the first email spam filter?\n",
      "\n",
      "You're off by about 9 years.  Bayesian classifiers didn't emerge as spam filters until approximately 1996.  They are currently believed to be first published by [Sahami et al in 1998](http://robotics.stanford.edu/users/sahami/papers-dir/spam.pdf).  That paper describes secretly internally using the technique in late 1996, and is the earliest known published discussion.  The internet at large caught on in 1999, just 22 years ago.\n",
      "\n",
      "The word SPAM actually comes from IRC and MUDs; we had spam filters long, long before email had spam, thanks to terminal washes and things of that nature.  The earliest known IRC spam filter was the `anarchy eris.berkeley.edu` stripper, which didn't work well enough, and led to the split of Jakko's original network to create eris-free net (EFnet is fundamentally named for a spam host removal.)\n",
      "\n",
      "If you count the invention of the q-line as an anti-spam strategy, then IRC invents spam filtering in 1991.  If you require message or origin testing, IRC invents it in 1992 instead.\n",
      "\n",
      "If you're old enough, you remember when Bayesian Filtering turned spam filtering from an ongoing joke into something that actually worked.  This was one of `gmail`'s early advantages.\n",
      "cuecat was a barcode scanner. Never did anything resembling text recognition. Nor were there any children's toys in the 90s or before that did anything of the sort (though they might do interesting stuff to convince *children* that they could!). And check recognition worked by \"cheating\" — first, using a special typeface with super easily distinguished characters and uniform size and spacing, and second, [printing it with magnetic ink](https://en.wikipedia.org/wiki/Magnetic_ink_character_recognition) so that the scanner didn't have to find the data it wanted among any kind of visual background. Everything except the routing and account numbers was invisible to it.\n",
      "> It is true that their findings did inspire creators of neocognitron [2] but that's about it.\n",
      "\n",
      "Uh, no, they're where that name comes from.\n",
      "\n",
      "What specific difference do you imagine exists between the neocognitron and CNNs?  They're both striding convolutions as a reduction for inputs.\n",
      "I'm not sure why you believe this.  Neural networks have been a big deal since the 1950s, taking down investments of half a billion at a time from the military for 70+ years now.\n",
      "Has this changed really ? :) In number of engineers with these skills, certainly, in proportion of developers, this remains to be seen. Python is the syntactic sugar but who goes really in and looks under the rug ?\n",
      "The assm skill was crazy back in the day! Nowadays I wouldn't use assm even with an 8bit microcontroller because I'm too lazy.\n",
      "Today's software are thousands times less efficient, because of all the overhead have been added layers on top of layes don't do any real work. Think about after all the closest, cabinets, drawers, boxes, organizers and wrappers, you still get the same pair of old socks and everyone cheers: \"Yeah! It works! We got the socks!\", that's what modern software actually is. But thank to these overhead, this industry have enough investment to support millions of overpaid software engineers, and most important of all, thousands of billionaires.\n",
      "CNN is robust to translation but not invariant to scale and rotation. Max pooling can be used to to combine detectors that trained for different scales and rotations.\n",
      "Did LeCunn make a lot of money from it?\n",
      "I don't doubt that his approach works, or his scientific integrity - simply that for each demo he might have loaded a different model for example (trained for different sizes or handwritten/typed text).\n",
      "This thread feels like r/KenM material\n",
      "At least you were born after color was invented, back in '53.\n",
      "Rich was already close to retirement at the time, so not really. Not sure about Donnie.\n",
      "Yann LeCun got the turing award for it\n",
      "I would struggle too\n",
      "Or \"What is god\"?\n",
      "That sounds fun. You have a site or a blog?\n",
      "I wish I could opt out. I don’t want to train skynet lol\n",
      "It greatly reduced the rate at which a bot can do whatever. With no delay something like filling out a form could probably be done thousands of times a second, but if you introduce a 0.1s delay by requiring some model to run then suddenly the maximum rate you can automatically fill out the same form is 10 times a second.\n",
      "\n",
      "Additionally, any more hurdles will naturally mean people need to be more sophisticated to get past them and you'll filter out a lot of the lowest effort bots.\n",
      "Also running a model involves computing costs\n",
      "In the wiki page (I put it at the right chapter) they state it was supposed to be \"omni-font\" as in reading all types of text, while *older* systems only recognized some set fonts. Note that there were already functional devices. Of course, those probably were of much worse quality than LeCun's small CNN, I just wanted to point out the person I'm responding to is full of shit.\n",
      "> Correct me if I'm wrong, but doesn't normal font imply a \"set font\"\n",
      "\n",
      "1. You're wrong\n",
      "1. Kurtzweil didn't invent this either\n",
      "1. The work being discussed here, the CNN, is actually from the late 1950s, from before LeCun was born\n",
      "**[Magnetic ink character recognition](https://en.wikipedia.org/wiki/Magnetic ink character recognition)**\n",
      "\n",
      "Magnetic ink character recognition code, known in short as MICR code, is a character recognition technology used mainly by the banking industry to streamline the processing and clearance of cheques and other documents. MICR encoding, called the MICR line, is at the bottom of cheques and other vouchers and typically includes the document-type indicator, bank code, bank account number, cheque number, cheque amount (usually added after a cheque is presented for payment), and a control indicator. The format for the bank code and bank account number is country-specific. The technology allows MICR readers to scan and read the information directly into a data-collection device.\n",
      "\n",
      "[^(About Me)](https://np.reddit.com/user/wikipedia_text_bot/comments/jrn2mj/about_me/) ^- [^(Opt out)](https://np.reddit.com/user/wikipedia_text_bot/comments/jrti43/opt_out_here/) ^(- OP can reply !delete to delete) ^- [^(Article of the day)](https://np.reddit.com/comments/k9hx22)\n",
      "\n",
      "**This bot will soon be transitioning to an opt-in system. Click [here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to learn more and opt in. Moderators: [click here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to opt in a subreddit.**\n",
      "NNs have definitely had a ton of research, so I agree that they weren't overlooked. However, up until 2012 they weren't very useful for most applications. Throughout the 2000s, SVMs and tree-based models (like random forests) were SOTA for most tasks. So most researchers put their focus there. \n",
      "\n",
      "2012 marked a transition though, as we then had the hardware support to efficiently train much larger models. This allowed NNs to become SOTA in many tasks and thus the explosion in interest\n",
      "I learned it here: https://youtu.be/uXt8qF2Zzfo\n",
      "In terms of what I intended to say, it's changed a lot. It wasn't an obvious career intially, so it caught a lot of people with a passion for it. The normal path for anyone who wanted visual output or realtime performance was to learn C and assembly. Operating systems were permissive, and memory mapping for access to video memory was either straightforward or documented well enough. Being able to do such things came with the job.. and if someone couldn't do it, that'd disqualify from a big chunk of the industry.\n",
      "\n",
      "I think you may have been referring to necessity being a great motivator.. and its converse -- that lack of necessity is a great blocker. Yep, I would agree. Lots of people in ML would now struggle somewhat with these basic graphical operations, even though the preparatory learning and experience required for it is now much less.\n",
      "I try to do and it is not pretty. Years of toil to make that one layer of cnn faster by inventing new winograd based algorithms. Working on the models are always more recognized.\n",
      "I think that's really cynical. Memory safe languages are a gigantic benefit to society in terms of security and stability.\n",
      "\n",
      "Such inefficiencies being permissible has allowed technology to flourish; a lot of programs would never have been written without being wasteful, see VS code vs Vim or Slack over IRC. IRC and Vim are nice cannot be mainstream and the only editor respectively. I don't see online web apps existing like Google Docs if everything had to be native speed fast. I've seen multiple homeless people with a card reader selling magazines, that's how cheap software has got over time that even homeless people have contactless.\n",
      "\n",
      "Arguably the progression of technology isn't what I'd have wanted to see but it isn't all bad. You can't help but wonder why something is slow on your 4GHz multicore CPU at times though haha.\n",
      "No, he was an employee at Bell Labs, the product and patents belonged to Bell Labs.\n",
      "\n",
      "When AT&T spun off Lucent in 1996, the patents went that way but the computer vision researchers stayed in the remaining AT&T Labs, and they couldn't even sell or improve the product without having the rights to the patents.\n",
      "\n",
      "LeCunn was an underdog for most of his life, the deep learning explosion only started happening around 2012 with AlexNet, when conv nets started getting all the attention.\n",
      "Here's a sneak peek of /r/KenM using the [top posts](https://np.reddit.com/r/KenM/top/?sort=top&t=year) of the year!\n",
      "\n",
      "\\#1: [KenM on billionaires](https://i.redd.it/tl38stlg70g41.jpg) | [164 comments](https://np.reddit.com/r/KenM/comments/f1j7a9/kenm_on_billionaires/)  \n",
      "\\#2: [Ken M on conspiracy theorists](https://i.redd.it/7inbjzicewo41.jpg) | [88 comments](https://np.reddit.com/r/KenM/comments/fp01kq/ken_m_on_conspiracy_theorists/)  \n",
      "\\#3: [One of my favorites over the years.](https://i.imgur.com/vjhwVXg.jpg) | [139 comments](https://np.reddit.com/r/KenM/comments/hmhwzo/one_of_my_favorites_over_the_years/)\n",
      "\n",
      "----\n",
      "^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/joo7mb/blacklist_viii/)\n",
      "big if true\n",
      "Automation can be a very secretive thing and very grey so I can't talk about projects or the details.\n",
      "Yup, that exact sentence speaks about normal fonts, I referred to.\n",
      ">  However, up until 2012 they weren't very useful for most applications.\n",
      "\n",
      "At that time, they were already in use by every call center and bank on earth, were running in every copy of Windows, MacOS, and Android, had dominated speech to text for almost 20 years, et cetera.\n",
      "\n",
      "Between Windows and MacOS, they were in over 50% of US homes.\n",
      "\n",
      "For color, ***The US phone system started using neural networks for de-noising in 1959, bringing their use to almost 200 million people***.\n",
      "\n",
      ".\n",
      "\n",
      "> 2012 marked a transition though, as we then had the hardware support to efficiently train much larger models.\n",
      "\n",
      "Respectfully, this is just kind of not true.\n",
      "I'm sorry, I'm not watching a 50 minute video to try to figure out why you believe that one of the world's largest intellectual pursuits was obscure or overlooked until an image recognition contest.\n",
      "\n",
      "My expectation is that whatever the video actually said was misunderstood.  Have a timestamp?\n",
      "\\>No, he was an employee at Bell Labs, the product and patents belonged to Bell Labs.\n",
      "\n",
      "I would just like to point out that in other countries (e.g. Germany, France Japan), inventors of a patent are entitled to a percentage of the revenue that this invention generates.\n",
      "\n",
      "This is not the case in the US, though.\n",
      "Oh yes, sorry. I think it's not a single set font, but at least several. But I also think you're right and this was made for printed fonts, so \"normal\" might mean \"very common fonts\".\n",
      "I'm not saying they weren't useful. They clearly had use cases as you mentioned. \n",
      "\n",
      "But if you look through ML papers you can clearly see an increase in interest after 2012. And in my experience as an ML engineer, there was a similar increase in interest on the business side after 2012 as well (though often lagging behind SOTA by a few years)\n",
      "He says it in the beginning of the video.\n",
      "I figured it's probably all printed fonts that aren't cursive or Comic Sans. You're definitely right that it's multiple, I think the limitation is just on the type of font.\n",
      "> But if you look through ML papers you can clearly see an increase in interest after 2012.\n",
      "\n",
      "ML papers still haven't caught up to their 1950s heyday, either in volume or in range.  As an issue of measurable fact, we continue to reel not just from the second AI winter, but also from the first.\n",
      "\n",
      "No, sir, today we are not inventing Lisp or Symbolics.  \n",
      "\n",
      "You keep saying SOTA.  This suggests to me that you're an internet fan.  Actual academics and actual industry people don't say that.\n",
      "\n",
      "Please have a good day.\n",
      "I watched the first three minutes.  I don't see anything supporting your claim, or any related evidence.  A timestamp would provide falsifiability, but you declined.\n",
      "\n",
      "There is ample evidence that these were being used by industry for decades, taught at thousands of universities, being discussed by the United Nations.\n",
      "\n",
      "Anyone who's ever seen Star Trek: TNG or Terminator 2 had seen them in the popular consciousness for decades at this point.\n",
      "\n",
      "Every bank had been using them for check scanning for 20+ years at the described point.\n",
      "\n",
      "There were more than a dozen instances where over a billion dollars was invested at a single time into the \"overlooked and forgotten until an image contest\" field.\n",
      "\n",
      "Please have a nice day.\n",
      "Clearly you haven't read many papers published in the last decade then. For better or worse, the term SOTA does show up in recent deep learning papers.... I've also definitely heard it used in my experience within industry as well. It's not super common, but that's a really weird thing to try to gatekeep on\n",
      "The opening concept is conveyed from 00:00 to 5:22.\n",
      "I'm sorry you keep ignoring the evidence and referring to wide swaths of time that do not seem to say what you claim.\n",
      "\n",
      "Claims are concrete.  If he actually says this, you should be able to give a timestamp.  I can't find it, and doubt your interpretation.\n",
      "\n",
      "Common sense says that even if he does say this, just looking at the contrary evidence would be enough to set him aside.  Mark Z Jacobsen is also a teacher at a prestigious university, y'know?  So is Scott Atlas.\n",
      "\n",
      "If the evidence disagrees with an academic, believe the evidence.  I can't even find the academic saying what you claim, and it seems like you can't either.\n",
      "\n",
      "Please have a good day.\n",
      "I was in the industry before 2012.  I have first hand experience.  I remember it too.  If you will not take it from an MIT professor teaching the topic, then who will you take it from?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MoreComments' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m top_comments\u001b[38;5;241m.\u001b[39mappend(com_list)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# examine the body of the text\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcom_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MoreComments' object has no attribute 'body'"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE  # the answer may vary 693 for r/machinelearning\n",
    "\n",
    "\n",
    "# for each submission in top_comments\n",
    "for submission in top_comments:\n",
    "    \n",
    "    # get the commentstree for each submision\n",
    "    comments_list = submission.comments.list()\n",
    "    \n",
    "    \n",
    "    # iterate through the list of comment tree lists\n",
    "    for com_list in comments_list:\n",
    "        \n",
    "        top_comments.append(com_list)\n",
    "        \n",
    "        # examine the body of the text\n",
    "        print(com_list.body)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_comments = [random.choice(top_comments) for i in range(3)]\n",
    "\n",
    "inspect_comments = []\n",
    "for comment in random_comments:\n",
    "    inspect_comments.append(comment.body)\n",
    "    \n",
    "inspect_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
    "\n",
    "    ['Awesome visualisation',\n",
    "    'Similar to a stack or connected neurons.',\n",
    "    'Will this Turing pass the Turing Test?']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "3. After having a chance to review a few samples of 5 comments from the subreddit, what can you say about the data? \n",
    "*A3: I would say that this data is quite messy and can be a bit difficult to handle. Especially since Reddit is constantly evoling this API, even such that one of the core functions in this tutorial was no longer valid. However, after some trouble shooting I was able to get access to the data that I needed for this assignemnt.*\n",
    "\n",
    "HINT: Think about the \"cleanliness\" of the data, the content of the data, think about what you're trying to do - how does this data line up with your goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# modify top ten func from above\n",
    "def mod_top_ten(subreddit_name):\n",
    "    \n",
    "    # get top ten posts from this week (it's a generator)\n",
    "    top_ten_gen = reddit.subreddit(f\"{subreddit_name}\").top(time_filter=\"week\",limit=10)\n",
    "\n",
    "    # init list for posts\n",
    "    posts = []\n",
    "\n",
    "    # get each post's title and author\n",
    "    for post in top_ten_gen:\n",
    "        posts.append(\n",
    "            post.selftext\n",
    "        ) \n",
    "    \n",
    "    return posts\n",
    "\n",
    "top_comments_tsla = mod_top_ten(\"TSLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments_tsla) # Expected: 174 for r/machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " \" Hey everyone,\\n\\nI'm looking for feedback on a new platform we have been working on all year. It's called MoneyAdx and it's an investment community where people can share information and advice.\\n\\nHere is a link to our space for conversations related to [TSLA](https://www.moneyadx.com/stock/tsla). \\n\\nI'm still working on the site and I'm open to any suggestions or feedback you might have. So please check it out and let me know what you think.\\n\\nSpecifically, please \\n\\n* Let us know what features we are missing that other websites have. \\n* Share your ideas on what would make this website more useful for you.\\n* Or anything else you think we should know! \\n\\nYou can either post your feedback here or on the website itself. \\n\\nThanks in advance! \\n\\nLooking forward to hearing from you,\\n\\nRichard H. \\n\\nFounder | MoneyAdx \\n\\nWe strive to empower our readers with the knowledge and skills necessary to take control of their financial future.\",\n",
       " \"Tesla TSLA Could Grow to a $4.5 Trillion Valuation Over the Next Decade, Says Ron Baron \\nBy: Eva Fox | November 4, 2022 \\n\\nTesla shares could rise 570% over the next decade to a $4.5 trillion valuation, according to investor Ron Baron. The estimate does not include robots, autonomous vehicles, nor batteries. \\n\\nRenowned investor Ron Baron of Baron Capital remains bullish on Tesla. He told CNBC on Friday that he expects Tesla to turn in huge gains that will eventually lead to a company valuation of around $4.5 trillion, up 570% from current levels. \\n\\nBaron is a longtime investor in Tesla, first acquiring a stake in the company in 2014. He has a combined stake in Tesla worth about $4 billion. The investor believes that in 2025, the company's shares will be worth about $500-600 each, and in 8-10 years, the total value of the company will be about $4.5 trillion. \\n\\n“I think in 2025 it [Tesla stock] will be $500 to $600. And in eight to ten years we ought to be somewhere around $4.5 trillion,” Baron said. \\n\\nThe investor agrees with Elon Musk's comments during the Q3 2022 Earnings Call that Tesla could be bigger than Apple and Saudi Aramco combined, implying a valuation of over $4 trillion. \\n\\nThe calculations behind Baron's bullish target are based on Tesla significantly expanding its business over the next decade, selling about 20 million vehicles a year. \\n\\n“In 2030, if he [Elon Musk] does 20 million cars per year, and they're $50,000 a car, that's $1 trillion in revenues, and he gets operating profits at somewhere around 30%,” Baron explained, adding a 15x multiple on those $300 billion in operating profits get you to $4.5 trillion. \\n\\nThe investor emphasizes that the bullish case outlined solely by the contribution from Tesla vehicles to revenue does not include other impressively profitable areas that the company is working on right now. Areas such as autonomous cars, robots, and batteries definitely offer a margin of safety for his investment. \\n\\n“But that's not including robots, that's not including autonomous vehicles, that's not including batteries. He [Musk] thinks robots are going to be bigger than cars. So there's so many things happening at Tesla,” it's hard to keep up, Baron explained.\",\n",
       " '']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(top_comments_tsla) for i in range(len(top_comments_tsla))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
    "\n",
    "    ['I bought puts',\n",
    "    '100%',\n",
    "    'Yes. And I’m bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "4. Now that you've had a chance to review another subreddits comments, do you see any differences in the kinds of comments either subreddit has - and how might this relate to bias?\n",
    "*A4: There were some major quality issues with the posts of TSLA. A lot of them were adds and spam*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task III: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Import `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\") # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "comment = random.choice(top_comments_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment = 'Bury Burry!!!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Make Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.989326000213623}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentiment = sentiment_model(comment) # YOUR CODE HERE \n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the type of the output `sentiment`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "The answer is an output of the label (positive or negative) and the score (0 to ~1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment: Bury Burry!!!!!\n",
      "Predicted Label is NEGATIVE and the score is 0.989\n"
     ]
    }
   ],
   "source": [
    "print(f'The comment: {comment}')\n",
    "print(f'Predicted Label is {sentiment[0][\"label\"]} and the score is {sentiment[0][\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example comment, the output is:\n",
    "\n",
    "    The comment: Bury Burry!!!!!\n",
    "    Predicted Label is NEGATIVE and the score is 0.989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🖥️❓ Model Question:\n",
    "\n",
    "1. What does the score represent?\n",
    "*A1: The score represents an accuracy score for the model. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Put All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the piece together, create a simple script that does \n",
    "\n",
    "- get the subreddit\n",
    "- get comments from the top posts for given subreddit\n",
    "- run sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_tlsa_comment_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_tlsa_comment_sentiment.py\n",
    "\n",
    "import reddit_secrets\n",
    "import random\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.subreddit import Subreddit\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_subreddit(display_name:str) -> Subreddit:\n",
    "    \"\"\"Get subreddit object from display name\n",
    "\n",
    "    Args:\n",
    "        display_name (str): [description]\n",
    "\n",
    "    Returns:\n",
    "        Subreddit: [description]\n",
    "    \"\"\"\n",
    "    reddit = Reddit(\n",
    "        client_id=reddit_secrets.REDDIT_API_CLIENT_ID,        \n",
    "        client_secret=reddit_secrets.REDDIT_API_CLIENT_SECRET,\n",
    "        user_agent=reddit_secrets.REDDIT_API_USER_AGENT\n",
    "        )\n",
    "    \n",
    "    subreddit = reddit.subreddit(\"TSLA\")\n",
    "    return subreddit\n",
    "\n",
    "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
    "    \"\"\" Get comments from subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit (Subreddit): [description]\n",
    "        limit (int, optional): [description]. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of comments\n",
    "    \"\"\"\n",
    "    top_comments = []\n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            top_comments.append(top_level_comment.body)\n",
    "    return top_comments\n",
    "\n",
    "def run_sentiment_analysis(comment:str) -> Dict:\n",
    "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
    "    \n",
    "    Args:\n",
    "        comment (str): [description]\n",
    "        \n",
    "    Returns:\n",
    "        str: Sentiment analysis result\n",
    "    \"\"\"\n",
    "    sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "    sentiment = sentiment_model(comment)\n",
    "    return sentiment[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    subreddit_display_name = \"TSLA\"\n",
    "    subreddit = get_subreddit(subreddit_display_name)\n",
    "    comments = get_comments(subreddit)\n",
    "    comment = [comments[0]]\n",
    "    sentiment = run_sentiment_analysis(comment)\n",
    "    \n",
    "    print(f'The comment: {comment}')\n",
    "    print(f'Predicted Label is {sentiment[\"label\"]} and the score is {sentiment[\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "The comment: ['ho lee fuk \\n\\nyou got anymore insider information? 👀👀']\n",
      "Predicted Label is NEGATIVE and the score is 0.994\n"
     ]
    }
   ],
   "source": [
    "!python top_tlsa_comment_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Expected output:</summary>\n",
    "\n",
    "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "    The comment: When is DOGE flying\n",
    "    Predicted Label is POSITIVE and the score is 0.689\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "5. Is the subreddit active? About how many posts or threads per day? How could you find this information?\n",
    "*This subreddit is not too active anymore. The TSLA subreddit is basically all adds*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💽❓ Data Question:\n",
    "\n",
    "6. Does there seem to be a large distribution of posters or a smaller concentration of posters who are very active? What kind of impact might this have on the data?\n",
    "\n",
    "*The posts seemed to be heavily created by bots and this would disrupt any real analysis of the subreddit.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-sa",
   "language": "python",
   "name": "reddit-sa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "c57794392b841cffd8686d5c4548e4e2ec78521f49300d60954d1380f1b4bd1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
